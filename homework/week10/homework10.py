## 1. 線性代數中的「線性」是什麼？為何叫「代數」？
線性（linearity）指：
一個運算 𝑇若對所有向量 𝑢,𝑣a,b 都滿足：
𝑇(𝑎𝑢+𝑏𝑣)=𝑎𝑇(𝑢)+𝑏𝑇(𝑣)就稱 線性。
➡️ 翻譯成直覺：
線性運算不會扭曲比例、不會打破加法結構，只會做伸縮、旋轉、投影等「均勻」變化。

# 為什麼叫「代數」？
因為線性代數研究的是：
向量的加法、倍數
線性變換
這些東西的運算規則（algebra）
「代數」表示：研究一組運算符則的系統。

## 2. 數學中的「空間」是什麼？為何向量空間是空間？
空間的直觀定義：

在數學中，「空間」表示：

一組元素 + 元素之間的某些結構（如距離、加法、內積…）

例如：

2D 平面是空間

3D 空間是空間

所有 
3
×
3
3×3 矩陣的集合也是空間（它們能做加法和倍數）

向量空間為何叫空間？

因為向量空間有兩個運算：

向量加法（封閉）

數乘（可以拉長縮短）

且滿足一系列公理。

因此它與我們熟悉的幾何空間行為「一樣」，能做：

平移（加法）

伸縮（倍數）

線性組合（拼出整個空間）

📌 3. 矩陣與向量的關係？矩陣代表什麼？
矩陣 = 線性變換的座標表示法

給定一個線性變換 
𝑇
T，它把向量 
𝑣
v 變成 
𝑇
(
𝑣
)
T(v)。
但若你指定了一組基底，這個變換可以用一個矩陣表示：

𝑇
(
𝑣
)
=
𝐴
𝑣
T(v)=Av
幾何圖像：

矩陣可以代表：

旋轉

縮放

剪切

投影

反射

矩陣的每一欄 = 基底向量被變換後的位置。

📌 4. 如何用矩陣表示 2D / 3D 的平移、縮放、旋轉？
❗ 注意：平移不是線性變換

因此需要「齊次座標」才能用矩陣表示。

✔️ 2D 縮放
𝑆
=
[
𝑠
𝑥
	
0


0
	
𝑠
𝑦
]
S=[
s
x
	​

0
	​

0
s
y
	​

	​

]
✔️ 2D 旋轉（角度 θ）
𝑅
(
𝜃
)
=
[
cos
⁡
𝜃
	
−
sin
⁡
𝜃


sin
⁡
𝜃
	
cos
⁡
𝜃
]
R(θ)=[
cosθ
sinθ
	​

−sinθ
cosθ
	​

]
✔️ 2D 平移（齊次座標）
𝑇
=
[
1
	
0
	
𝑡
𝑥


0
	
1
	
𝑡
𝑦


0
	
0
	
1
]
T=
	​

1
0
0
	​

0
1
0
	​

t
x
	​

t
y
	​

1
	​

	​


3D 的形式類似，只是多一維。

📌 5. 行列式的意義？與體積關係？遞迴公式？
✔️ 行列式意義（幾何）

矩陣 
𝐴
A 的行列式 
det
⁡
(
𝐴
)
det(A) = 空間體積被 A 縮放的倍數。

若 
∣
det
⁡
𝐴
∣
=
3
∣detA∣=3：體積變 3 倍

若 
det
⁡
𝐴
=
0
detA=0：空間被壓扁，維度下降（不可逆）

✔️ 遞迴公式（Laplace 展開）：

計算第 1 行：

det
⁡
(
𝐴
)
=
∑
𝑗
=
1
𝑛
(
−
1
)
1
+
𝑗
𝑎
1
𝑗
𝑀
1
𝑗
det(A)=
j=1
∑
n
	​

(−1)
1+j
a
1j
	​

M
1j
	​


其中 
𝑀
1
𝑗
M
1j
	​

 是刪掉第 1 行第 j 欄後的子矩陣的行列式。

📌 6. 如何透過對角化快速計算行列式？

若矩陣可對角化：

𝐴
=
𝑃
𝐷
𝑃
−
1
A=PDP
−1

則

det
⁡
(
𝐴
)
=
det
⁡
(
𝑃
)
det
⁡
(
𝐷
)
det
⁡
(
𝑃
−
1
)
=
det
⁡
(
𝐷
)
det(A)=det(P)det(D)det(P
−1
)=det(D)

因為 
det
⁡
(
𝑃
)
det
⁡
(
𝑃
−
1
)
=
1
det(P)det(P
−1
)=1。

而對角矩陣的行列式就是對角線相乘：

det
⁡
(
𝐷
)
=
∏
𝑖
𝜆
𝑖
det(D)=
i
∏
	​

λ
i
	​


也就是 所有特徵值的乘積。

📌 7. 如何用 LU 分解計算行列式？

若

𝐴
=
𝐿
𝑈
A=LU

其中：

𝐿
L 是下三角矩陣（對角多為 1）

𝑈
U 是上三角矩陣

則：

det
⁡
(
𝐴
)
=
det
⁡
(
𝐿
)
det
⁡
(
𝑈
)
=
∏
𝑖
𝑢
𝑖
𝑖
det(A)=det(L)det(U)=
i
∏
	​

u
ii
	​


因為 
det
⁡
(
𝐿
)
=
1
det(L)=1。

📌 8. 特徵值與特徵向量的意義？用途？
✔️ 幾何意義

特徵向量是「變換方向不變的向量」：

𝐴
𝑣
=
𝜆
𝑣
Av=λv

矩陣作用後：

方向不變

只被拉伸 λ 倍

✔️ 用途

對角化矩陣

快速計算 
𝐴
𝑛
A
n

微分方程

主成分分析（PCA）

物理中的正交振動模式

Markov 鏈的穩態

📌 9. QR 分解是什麼？

將矩陣拆成：

𝐴
=
𝑄
𝑅
A=QR

其中：

𝑄
Q：正交矩陣（代表旋轉）

𝑅
R：上三角

主要用途：

數值穩定的解線性方程

特徵值計算的基礎

📌 10. 如何透過反覆 QR 分解做特徵值分解？（QR 演算法）

反覆做：

𝐴
𝑘
+
1
=
𝑅
𝑘
𝑄
𝑘
A
k+1
	​

=R
k
	​

Q
k
	​


其中 
𝐴
𝑘
=
𝑄
𝑘
𝑅
𝑘
A
k
	​

=Q
k
	​

R
k
	​

 是 QR 分解。

最終：

𝐴
𝑘
→
上三角矩陣
A
k
	​

→上三角矩陣

上三角矩陣的對角線即特徵值。

這是現代數值線性代數求特徵值的主流方法。

📌 11. SVD 分解是什麼？與特徵值分解的關係？
✔️ SVD 定義
𝐴
=
𝑈
Σ
𝑉
𝑇
A=UΣV
T

其中：

𝑈
,
𝑉
U,V 為正交矩陣

Σ
Σ 為對角矩陣（奇異值）

✔️ 與特徵值分解的關係
𝐴
𝑇
𝐴
=
𝑉
Σ
2
𝑉
𝑇
A
T
A=VΣ
2
V
T

奇異值 = 
𝐴
𝑇
𝐴
A
T
A 的特徵值平方根。

📌 12. 主成分分析（PCA）是什麼？與 SVD 的關係？
✔️ PCA 的目標

找出資料分布中方向，使投影後：

變異最大

資訊保留最多

維度最少

✔️ 與 SVD 的關係

資料矩陣 
𝑋
X 做 SVD：

𝑋
=
𝑈
Σ
𝑉
𝑇
X=UΣV
T

𝑉
V：主成分方向（特徵向量）

Σ
Σ：方向的重要程度（奇異值）

選前 k 個主成分 = 選取最大的 k 個奇異值

所以：

PCA = SVD 應用於資料矩陣
